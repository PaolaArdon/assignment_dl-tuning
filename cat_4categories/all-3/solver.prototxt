##################################################
## to be modified/checked for each experiment
##################################################

# number of training iterations
max_iter: FILLME
# QUESTION: how did we set this parameter in the tutorial?
# ANSWER:
# 1. count the number of images in train.txt
# 2. look inside the file train_val.prototxt and check the size of the mini-batches in train phase
# 3. this means that each epoch of the mini-batch gradient descent (an epoch is a pass over all training images)
#    corresponds to ... iterations
# 4. if we want to train for 4 epochs then we need to set <max_iter> to ...

# number of iterations to be performed at each validation step
test_iter: FILLME
# QUESTION: how did we set this parameter in the tutorial?
# ANSWER:
# 1. count the number of images in val.txt
# 2. look inside the file train_val.prototxt and check the size of the mini-batches in the test (i.e. validation) phase
# 3. this means that a pass over all validation images
#    corresponds to ... iterations
# 4. if we want (clearly) to perform a single pass over the validation set at each validation step then <test_iter> will be ...

# carry out a validation step every <test_interval> training iterations
test_interval: FILLME
# we perform a validation step every epoch (i.e. every ... iterations)

# display performance every <display> iterations
display: FILLME
# we display every epoch (i.e. every ... iterations)

##################################################
## the following can be left as they are
##################################################

# relative path to the network definition
net: "train_val.prototxt"

# CPU/GPU mode
solver_mode: CPU

# save a snapshot every <snapshot> iterations
snapshot: 0
# we do not want to save intermediate snapshots

# the prefix of the name of the snapshot models (the suffix is the iteration number at which the model is saved)
snapshot_prefix: "icw"

# solver type
solver_type: SGD
momentum: 0.9

# L2 regularization
weight_decay: 0.0005

# learning rate decay policy
base_lr: 0.001
lr_policy: "poly"
power: 0.5

# NOTE: the currently implemented learning rate
# policies are:
#
#    - fixed: always return base_lr.
#    - step: return base_lr * gamma ^ (floor(iter / stepsize))
#    - exp: return base_lr * gamma ^ iter
#    - inv: return base_lr * (1 + gamma * iter) ^ (- power)
#    - multistep: similar to step but it allows non uniform steps defined by
#      stepvalue
#    - poly: the effective learning rate follows a polynomial decay, to be
#      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)
#    - sigmoid: the effective learning rate follows a sigmod decay
#      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))
#
# where base_lr, max_iter, gamma, stepsize, stepvalue and power are defined
# in the solver parameter protocol buffer, and iter is the current iteration.
